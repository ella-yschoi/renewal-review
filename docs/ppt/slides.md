---
theme: seriph
background: https://cover.sli.dev
title: "Agent-Native Engineering: Renewal Review"
info: |
  Insurance renewal review pipeline built with AI agents.
  Quandri interview presentation â€” Feb 2026.
class: text-center
highlighter: shiki
colorSchema: dark
transition: fade-out
mdc: true
lineNumbers: false
drawings:
  persist: false
---

# Agent-Native Engineering

### Insurance Renewal Review Pipeline

<div class="pt-4 text-gray-400">
8,000 Policies Â· Rule + LLM Hybrid Â· 5x Faster
</div>

<div class="pt-6 text-left max-w-2xl mx-auto text-sm text-gray-400 leading-relaxed">

**My Goal**: Set up an agent-native environment, plan thoroughly, then let AI agents execute â€” compressing a full work week into a day. Along the way, run experiments to find what works and what doesn't, and package the results into reusable workflows for the team.

</div>

<div class="abs-br m-6 flex gap-2 text-sm text-gray-500">
  Yeonsu Choi Â· Feb 2026
</div>

<!--
ì˜¤í”„ë‹: "8,000ê±´ ë³´í—˜ ê°±ì‹  ì •ì±…ì„ ìë™ìœ¼ë¡œ ì‹¬ì‚¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
ëª©í‘œëŠ” ì„¸ ê°€ì§€ì˜€ìŠµë‹ˆë‹¤:
1) agent-native í™˜ê²½ì„ ì…‹ì—…í•˜ê³ , ì¶©ë¶„íˆ ê³„íší•œ ë‹¤ìŒ, AIê°€ ì‹¤í–‰í•˜ê²Œ í•´ì„œ 5ì¼ ê±¸ë¦´ ì‘ì—…ì„ í•˜ë£¨ì— ëë‚´ê¸°
2) ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ AI ì›Œí¬í”Œë¡œìš°ê°€ íš¨ê³¼ì ì¸ì§€ ì‹¤í—˜í•˜ê¸°
3) ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒ€ì´ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Skillê³¼ ê°€ì´ë“œë¡œ íŒ¨í‚¤ì§•í•˜ê¸°
ì´ ë°œí‘œì—ì„œëŠ” ë¬´ì—‡ì„ ë§Œë“¤ì—ˆëŠ”ì§€, ì–¼ë§ˆë‚˜ ë¹¨ëëŠ”ì§€, ê·¸ë¦¬ê³  5ê°€ì§€ ì‹¤í—˜ì˜ ê²°ê³¼ë¥¼ ê³µìœ í•˜ê² ìŠµë‹ˆë‹¤."
-->

---
layout: center
class: text-center
---

# The Problem

<div class="text-2xl text-gray-400 py-4">
Brokers manually review 8,000 renewal policies every season
</div>

<div class="grid grid-cols-3 gap-8 pt-8">
<div>
<div class="text-4xl font-bold text-red-400">Days</div>
<div class="text-gray-400 text-sm">of manual comparison</div>
</div>
<div>
<div class="text-4xl font-bold text-red-400">15+</div>
<div class="text-gray-400 text-sm">risk categories to check</div>
</div>
<div>
<div class="text-4xl font-bold text-red-400">Missed</div>
<div class="text-gray-400 text-sm">text-based risk signals</div>
</div>
</div>

<div class="pt-10 text-lg text-gray-300">

My Approach: &nbsp; **Setup** â†’ **Plan** â†’ **Build** â†’ **Experiment** â†’ **Iterate**

</div>

<!--
"ë³´í—˜ ê°±ì‹  ì‹œì¦Œì— ë¸Œë¡œì»¤ê°€ ìˆ˜ì²œ ê±´ì˜ ì •ì±…ì„ í•˜ë‚˜í•˜ë‚˜ ë¹„êµí•˜ëŠ”ë° ë©°ì¹ ì´ ê±¸ë¦½ë‹ˆë‹¤.
15ê°œ ì´ìƒì˜ ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë¥¼ ëˆˆìœ¼ë¡œ ì²´í¬í•´ì•¼ í•˜ê³ , ë©”ëª¨ë‚˜ íŠ¹ì•½ í…ìŠ¤íŠ¸ì— ìˆ¨ì–´ìˆëŠ” ìœ„í—˜ ì‹ í˜¸ëŠ” ë†“ì¹˜ê¸° ì‰½ìŠµë‹ˆë‹¤.
ì €ì˜ ì ‘ê·¼ ë°©ì‹ì€ Setup â†’ Plan â†’ Build â†’ Experiment â†’ Iterate.
ë¨¼ì € í™˜ê²½ì„ ì„¸íŒ…í•˜ê³ , ê³„íšì„ ì„¸ìš°ê³ , ë¹ ë¥´ê²Œ ë§Œë“¤ê³ , ì‹¤í—˜í•˜ê³ , ë°˜ë³µí•©ë‹ˆë‹¤.
ì´ íë¦„ëŒ€ë¡œ ë°œí‘œë¥¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤."
-->

---

# What I Built

<div class="grid grid-cols-2 gap-4">
<div>

```mermaid {scale: 0.75}
flowchart TD
    A["8,000 Policies<br/>(JSON / PostgreSQL)"] --> B["Parser<br/>ACORD field normalization"]
    B --> C["Diff Engine<br/>Prior vs Renewal comparison"]
    C --> D["Rule Flagger<br/>15 threshold rules â†’ DiffFlags"]
    D --> E["Risk Classifier<br/>4-level risk assignment"]
    E --> F{"Text changed?<br/>5-15% of policies"}
    F -->|Yes| G["LLM Analyzer<br/>Notes Â· Endorsements"]
    F -->|No| H["Risk Aggregator"]
    G --> H
    H --> I["Review Result<br/>+ Quotes + Portfolio"]
```

</div>
<div class="pl-4">

### Features

- **Dashboard** â€” batch processing, risk distribution, policy list
- **Review Detail** â€” prior vs renewal side-by-side, flags, AI insights
- **Analytics** â€” batch history, trend charts
- **Quote Generator** â€” 5 cost-saving strategies per policy type
- **Portfolio Analyzer** â€” cross-policy bundle analysis, duplication detection
- **LLM Insights** â€” risk signals from notes, endorsement comparison, personalized broker tips

<div class="pt-4 text-sm text-gray-400">

100 tests Â· 14+ API endpoints Â· 7 UI pages Â· 8,000 policies < 1 second

</div>
</div>
</div>

<!--
"ì™¼ìª½ì´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. êµ¬ì¡°í™”ëœ í•„ë“œëŠ” 100% rule-basedë¡œ ì²˜ë¦¬í•˜ê³ ,
ë¹„ì •í˜• í…ìŠ¤íŠ¸(ë©”ëª¨, íŠ¹ì•½)ë§Œ LLMì— ì„ ë³„ íˆ¬ì…í•©ë‹ˆë‹¤. ì „ì²´ ì •ì±…ì˜ 5-15%ë§Œ LLMì„ í˜¸ì¶œí•˜ë¯€ë¡œ ë¹„ìš© íš¨ìœ¨ì ì…ë‹ˆë‹¤.
ì˜¤ë¥¸ìª½ì€ ì£¼ìš” ê¸°ëŠ¥ â€” ëŒ€ì‹œë³´ë“œ, ê°œë³„ ë¦¬ë·°, ë¶„ì„, ê²¬ì , í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ í¬í•¨í•©ë‹ˆë‹¤."
-->

---
layout: center
---

# The Speed Story

<div class="grid grid-cols-3 gap-12 items-center pt-8">
<div class="text-center">
<div class="text-6xl font-bold text-gray-500">5 days</div>
<div class="text-gray-500 pt-2">Manual Development<br/>(~37 hours estimated)</div>
</div>
<div class="text-center">
<div class="text-5xl">â†’</div>
</div>
<div class="text-center">
<div class="text-6xl font-bold text-green-400">1 day</div>
<div class="text-green-400/70 pt-2">Core System with AI<br/>(~4 hours Â· 5x speedup)</div>
</div>
</div>

<div class="pt-8 text-center text-sm text-gray-400">
+ 5 experiments on top â†’ total project completed in ~2 days
</div>

<div class="pt-4 text-center">

| Phase | AI Agent | Manual Dev | Speedup |
|-------|----------|------------|---------|
| Models + Parser (ACORD) | 30 min | 4h | 8x |
| Diff Engine + 15 Rules | 45 min | 6h | 8x |
| Mock Data (8,000 policies) | 20 min | 3h | 9x |
| LLM Client + 4 Prompts | 30 min | 5h | 10x |
| Batch + API + Frontend | 75 min | 10h | 8x |

</div>

<!--
"ì‹œë‹ˆì–´ ê°œë°œì ê¸°ì¤€ìœ¼ë¡œ ì‚°ì •í•˜ë©´ ì´ ì‹œìŠ¤í…œì€ ìˆœìˆ˜ ê°œë°œë¡œ ì•½ 37ì‹œê°„, 5ì¼ ê±¸ë¦¬ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
AI agentë¡œ ì½”ì–´ ì‹œìŠ¤í…œì„ í•˜ë£¨ ë§Œì— ì™„ì„±í–ˆìŠµë‹ˆë‹¤. 5ë°° ë¹ ë¦…ë‹ˆë‹¤.
ê±°ê¸°ì— 5ê°€ì§€ ì‹¤í—˜ â€” agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜, ì‚¼ê°ê²€ì¦, ìê°€ìˆ˜ì • ë£¨í”„, LLM ë²¤ì¹˜ë§ˆí¬ â€” ê¹Œì§€ í¬í•¨í•´ì„œ ì´ 2ì¼ì— ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.
ê°€ì¥ í° ì‹œê°„ ì ˆì•½ì€ ë„ë©”ì¸ ë¦¬ì„œì¹˜ì…ë‹ˆë‹¤. ACORD ë³´í—˜ í‘œì¤€ì„ ë³„ë„ í•™ìŠµ ì—†ì´ ë°”ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
-->

---

# How: Agent-Native Setup

<div class="grid grid-cols-2 gap-8">
<div>

### Rulesets â€” Agent Behavior Control

```yaml
# CLAUDE.md (loaded every session)
- No docstrings â†’ clear naming + type hints
- Read convention.md before every change
- Minimal diffs only
- Token-aware: grep/glob before full reads
- Pre-commit: Ruff + Gitleaks + Semgrep
```

```yaml
# convention.md
- Files < 300 lines (split when approaching)
- Hexagonal layer rules (domain/ imports nothing)
- StrEnum for finite values, Config for thresholds
- Immutable models, DI via Depends()
```

</div>
<div>

### Quality Gates â€” Reward Signals

| Gate | Purpose |
|------|---------|
| **pytest** (100 tests) | Agent iterates until all pass |
| **Ruff** | Lint + format â€” prevents code slop |
| **Gitleaks** | Blocks secrets in commits |
| **Semgrep** | SAST security scanning |
| **Hypothesis** | Property-based edge case testing |

### Claude Code Hooks

| Hook | Type | Action |
|------|------|--------|
| `require-experiment-log` | PreToolUse | Blocks commit without experiment log |
| `require-design-doc` | PreToolUse | Blocks commit if code changed without doc update |
| `remind-design-doc` | PostToolUse | One-time reminder on code edit |

</div>
</div>

<!--
"Agent-nativeì˜ í•µì‹¬ì€ agentì—ê²Œ ì½”ë“œë¥¼ ë§¡ê¸°ëŠ” ê²Œ ì•„ë‹ˆë¼, agentê°€ ì˜ ì¼í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ë¨¼ì € ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.
CLAUDE.mdì™€ convention.mdë¡œ í–‰ë™ ê·œì¹™ì„ ì •ì˜í•˜ê³ , í’ˆì§ˆ ê²Œì´íŠ¸(í…ŒìŠ¤íŠ¸, ë¦°í„°, ë³´ì•ˆ ìŠ¤ìºë„ˆ)ë¥¼ agentì˜ reward signalë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
ì»¤ë°‹í•  ë•Œë§ˆë‹¤ ìë™ìœ¼ë¡œ ì „ë¶€ ì‹¤í–‰ë˜ê³ , í†µê³¼í•˜ì§€ ëª»í•˜ë©´ ì»¤ë°‹ ìì²´ê°€ ë¶ˆê°€í•©ë‹ˆë‹¤.
ì¶”ê°€ë¡œ Claude Code Hook 3ê°œë¥¼ ë§Œë“¤ì–´ì„œ â€” ì‹¤í—˜ ë¡œê·¸ ì—†ì´ ì»¤ë°‹ ë¶ˆê°€, ì½”ë“œ ë³€ê²½ ì‹œ design-doc ì—†ì´ ì»¤ë°‹ ë¶ˆê°€ â€” ë¬¸ì„œ ì—…ë°ì´íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ê°•ì œí•©ë‹ˆë‹¤."
-->

---

# Documentation-Driven Planning

<div class="grid grid-cols-3 gap-6">
<div>

### requirements.md
- FR-1 ~ FR-9 functional requirements
- Success criteria with numbers
- Golden eval scenarios (5 cases)
- Non-functional: <10s for 8,000 policies

</div>
<div>

### design-doc.md
- 5-layer hexagonal architecture
- Data model (8 Pydantic models)
- 15 DiffFlags, 4 risk levels
- API surface (14+ endpoints)
- Auto-updated via hooks

</div>
<div>

### implementation-plan.md
- Phase 0-2C execution roadmap
- Per-phase: files, line estimates, commit message
- V1 (rule) â†’ V2 (LLM) progressive migration
- Feature flag: `RR_LLM_ENABLED`

</div>
</div>

<div class="pt-6 text-center text-gray-400">

*"I didn't ask the agent to 'build me an insurance system.' I gave it a phased plan with exact file names, function signatures, and verification criteria at each step."*

</div>

<!--
"ì½”ë“œë¥¼ ì“°ê¸° ì „ì— ë¬¸ì„œ 3ê°œë¥¼ ë¨¼ì € ì‘ì„±í–ˆìŠµë‹ˆë‹¤.
requirements.mdì— ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ê³¼ ì„±ê³µ ê¸°ì¤€, design-doc.mdì— ì•„í‚¤í…ì²˜ì™€ ë°ì´í„° ëª¨ë¸,
implementation-plan.mdì— Phaseë³„ ì‹¤í–‰ ê³„íš.
Agentì—ê²Œ 'ë³´í—˜ ì‹œìŠ¤í…œ ë§Œë“¤ì–´ì¤˜'ê°€ ì•„ë‹ˆë¼ 'ì´ ìˆœì„œë¡œ, ì´ êµ¬ì¡°ë¡œ, ì´ ê¸°ì¤€ì„ í†µê³¼í•˜ê²Œ' ì§€ì‹œí•©ë‹ˆë‹¤.
ê·¸ë¦¬ê³  design-docì€ ì½”ë“œ ë³€ê²½ ì‹œ í›…ìœ¼ë¡œ ìë™ ì—…ë°ì´íŠ¸ë¥¼ ê°•ì œí•©ë‹ˆë‹¤."
-->

---
layout: center
class: text-center
---

# Five Experiments

<div class="text-gray-400 pb-6">
Each experiment answered a specific question about AI-assisted development
</div>

```mermaid {scale: 0.8}
flowchart LR
    E1["<b>Exp 1</b><br/>SubAgent vs<br/>Agent Teams"] --> E2["<b>Exp 2</b><br/>Triangular<br/>Verification"]
    E2 --> E3["<b>Exp 3</b><br/>Self-Correcting<br/>Loop"]
    E3 --> E4["<b>Exp 4</b><br/>Pipeline<br/>Reusability"]
    E4 --> E5["<b>Exp 5</b><br/>Langfuse LLM<br/>Benchmark"]

    style E1 fill:#1e3a5f,stroke:#4a9eff
    style E2 fill:#1e3a5f,stroke:#4a9eff
    style E3 fill:#2d5a1e,stroke:#4aff4a
    style E4 fill:#2d5a1e,stroke:#4aff4a
    style E5 fill:#5a3a1e,stroke:#ffaa4a
```

<div class="grid grid-cols-5 gap-4 pt-4 text-xs text-gray-400">
<div class="text-center">"Can I run multiple agents?"</div>
<div class="text-center">"Can agents verify each other?"</div>
<div class="text-center">"Can verifyâ†’fix be automated?"</div>
<div class="text-center">"Does the pipeline generalize?"</div>
<div class="text-center">"Which LLM is best for this?"</div>
</div>

<!--
"5ê°€ì§€ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰í–ˆê³ , ê° ì‹¤í—˜ì€ ì´ì „ ì‹¤í—˜ì˜ ê²°ê³¼ ìœ„ì— ìŒ“ì…ë‹ˆë‹¤.
ì‹¤í—˜ 1: ì—¬ëŸ¬ agentë¥¼ ë™ì‹œì— ëŒë¦´ ìˆ˜ ìˆëŠ”ê°€?
ì‹¤í—˜ 2: agentë¼ë¦¬ ì„œë¡œ ê²€ì¦í•  ìˆ˜ ìˆëŠ”ê°€?
ì‹¤í—˜ 3: ê²€ì¦ë¶€í„° ìˆ˜ì •ê¹Œì§€ ìë™í™”í•  ìˆ˜ ìˆëŠ”ê°€?
ì‹¤í—˜ 4: ê·¸ íŒŒì´í”„ë¼ì¸ì´ ë‹¤ë¥¸ ê¸°ëŠ¥ì—ì„œë„ ì¬ì‚¬ìš© ê°€ëŠ¥í•œê°€?
ì‹¤í—˜ 5: ì–´ë–¤ LLM providerê°€ ì´ ë„ë©”ì¸ì— ìµœì ì¸ê°€?"
-->

---

# Exp 1: SubAgent vs Agent Teams

<div class="text-sm text-gray-400 pb-2">Same task (Analytics module, ~300 lines) Â· Two orchestration patterns</div>

<div class="grid grid-cols-2 gap-8">
<div>

### SubAgent Pattern
```
Orchestrator
  â”œâ”€ [1] Research subagent (Explore)
  â”œâ”€ [2] Model + Service â”€â”
  â”‚                        â”œâ”€ parallel
  â”œâ”€ [3] Routes + Main â”€â”€â”€â”˜
  â””â”€ [4] Tests (after 2,3)
```
**Key**: Orchestrator specifies interface specs in prompt â†’ parallel dispatch possible

</div>
<div>

### Agent Teams Pattern
```
Team Lead
  â”œâ”€ TaskCreate: #1 â†’ #2 â†’ #3
  â”œâ”€ [spawn] modeler  â†’ task #1
  â”œâ”€ [spawn] router   â†’ task #2
  â””â”€ [spawn] tester   â†’ task #3
```
**Key**: Explicit dependency (blockedBy) Â· Each member reads conventions independently

</div>
</div>

<div class="pt-4">

| Metric | SubAgent | Agent Teams |
|--------|----------|-------------|
| Time | 354s (~6 min) | 318s (~5 min) |
| Code generated | 334 lines, 8 files | 335 lines, 8 files |
| Tests | 73 all pass | 73 all pass |
| Lint fixes needed | 1 (ruff format) | 0 |

</div>

<div class="pt-2 text-sm">

> **Insight**: Small tasks (~300 lines) â†’ SubAgent is simpler. Agent Teams shines at scale with complex dependencies.
> **Limitation**: Task was too small for Teams' advantages. A 1,000+ line multi-module task would differentiate better.

</div>

<!--
"ë™ì¼í•œ ê³¼ì œ â€” Analytics ëª¨ë“ˆ ì¶”ê°€ â€” ë¥¼ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.
SubAgentëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ ì¸í„°í˜ì´ìŠ¤ ìŠ¤í™ì„ í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œí•´ì„œ ë³‘ë ¬ ë””ìŠ¤íŒ¨ì¹˜í•©ë‹ˆë‹¤.
Agent TeamsëŠ” TaskCreateë¡œ íƒœìŠ¤í¬ë¥¼ ë“±ë¡í•˜ê³  ì˜ì¡´ì„±ì„ ì„¤ì •í•©ë‹ˆë‹¤.
ê²°ê³¼ëŠ” ê±°ì˜ ë™ì¼ â€” 6ë¶„ ì•ˆì— í”„ë¡œë•ì…˜ ë ˆë”” ëª¨ë“ˆì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.
ì°¨ì´ê°€ ì—†ë‹¤ëŠ” ê²ƒ ìì²´ê°€ ì¸ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤. ì†Œê·œëª¨ì—ì„œëŠ” SubAgentê°€ ì‹¤ìš©ì ì´ê³ ,
ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ì—ì„œ Teamsì˜ íƒœìŠ¤í¬ ì¶”ì /ì˜ì¡´ì„± ê´€ë¦¬ê°€ ë¹›ë‚  ê²ƒì…ë‹ˆë‹¤.
í•œê³„ë¥¼ ê°œì„ í•œë‹¤ë©´, 1000ì¤„ ì´ìƒì˜ ë©€í‹°ëª¨ë“ˆ ê³¼ì œë¡œ ì¬ì‹¤í—˜í•˜ë©´ Teamsì˜ ì¥ì ì´ ë“œëŸ¬ë‚  ê²ƒì…ë‹ˆë‹¤."
-->

---

# Exp 2: Triangular Verification â€” Concept

<div class="text-sm text-gray-400 pb-2">3-agent information isolation catches what linters and tests cannot</div>

```mermaid {scale: 0.85}
flowchart LR
    subgraph "Information Isolation"
        direction TB
        R["ğŸ“‹ Requirements<br/>(original spec)"]
        B["ğŸ” Agent B<br/>Reads: code + convention<br/><b>Cannot see: requirements</b>"]
        C["âš–ï¸ Agent C<br/>Reads: requirements + B's review<br/><b>Cannot see: code</b>"]
    end

    Code["ğŸ’» Code<br/>(Agent A output)"] --> B
    B -->|"blind-review.md<br/>describes what code does"| C
    R --> C
    C -->|"discrepancy-report.md<br/>requirement vs actual gaps"| Result{"PASS / FAIL"}
```

<div class="pt-4">

### Why This Works

| Tool | Catches | Misses |
|------|---------|--------|
| **Ruff** | Syntax, formatting, imports | Intent mismatch |
| **Pytest** | Logic bugs (if test exists) | Missing features |
| **Semgrep** | Security patterns | Business logic gaps |
| **Triangular** | **Intent mismatch**, missing features, extra features | UI-level issues |

</div>

<div class="pt-2 text-sm text-gray-400">

*"The key insight: Agent B doesn't know what the code should do. It describes what the code actually does. Agent C compares that description against requirements â€” without code bias."*

</div>

<!--
"ì‚¼ê° ê²€ì¦ì˜ í•µì‹¬ì€ ì •ë³´ ê²©ë¦¬ì…ë‹ˆë‹¤.
Agent BëŠ” ì½”ë“œì™€ ì»¨ë²¤ì…˜ë§Œ ë³´ê³  'ì´ ì½”ë“œê°€ ë­˜ í•˜ëŠ”ì§€' ì„¤ëª…í•©ë‹ˆë‹¤. ìš”êµ¬ì‚¬í•­ì€ ë³´ì§€ ëª»í•©ë‹ˆë‹¤.
Agent CëŠ” ìš”êµ¬ì‚¬í•­ê³¼ Bì˜ ì„¤ëª…ë§Œ ë³´ê³  'ë¶ˆì¼ì¹˜ê°€ ìˆëŠ”ê°€' íŒë‹¨í•©ë‹ˆë‹¤. ì½”ë“œëŠ” ë³´ì§€ ëª»í•©ë‹ˆë‹¤.
ì´ ì„¸ ê´€ì  â€” ìš”êµ¬ì‚¬í•­, ì½”ë“œ, ë…ë¦½ ë¶„ì„ â€” ì´ ì¼ì¹˜í•˜ë©´ PASSì…ë‹ˆë‹¤.
ê¸°ì¡´ ë„êµ¬(ruff, pytest, semgrep)ëŠ” êµ¬ë¬¸ê³¼ ë³´ì•ˆë§Œ ì²´í¬í•©ë‹ˆë‹¤.
ì‚¼ê° ê²€ì¦ì€ 'ì˜ë„ëŒ€ë¡œ ë§Œë“¤ì—ˆëŠ”ê°€'ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."
-->

---

# Exp 2: Triangular Verification â€” Results

<div class="text-sm text-gray-400 pb-2">Applied to Analytics module (already passing all linters + tests)</div>

<div class="grid grid-cols-2 gap-8">
<div>

### Issues Found

| Category | Count | Example |
|----------|-------|---------|
| Intent Mismatch | 2 | FIFO 100-entry limit not implemented |
| Missing Feature | 2 | History endpoint behavior gap |
| Extra Feature | 3 | Undocumented features in code |
| Convention Violation | 2 | Timezone hardcoding mismatch |
| False Positive | 2 | â€” |

<div class="pt-4">

| Metric | Value |
|--------|-------|
| **Precision** | **78%** (7/9 true positives) |
| **Standard tools found** | 0 issues |
| **Triangular found** | 9 issues |
| **Time cost** | ~19 min |

</div>
</div>
<div>

### Critical Finding: FIFO 100-Entry Limit

Requirements said: *"Maximum 100 entries, FIFO"*

Code: unlimited `append()` â€” no cap

ruff: âœ… &nbsp; pytest: âœ… &nbsp; semgrep: âœ…

**Triangular: âŒ FOUND IT**

<div class="pt-6 text-sm">

### Limitations & Improvements

- Agent B can only analyze Python code â€” **UI/template verification is blind**
- 78% precision means ~22% false positives â€” could improve with more specific requirements
- ~19 min overhead per run â€” **worth it for critical features, not every commit**
- **Next step**: Add template/frontend analysis skill to Agent B

</div>
</div>
</div>

<!--
"ì´ë¯¸ ruff, pytest, semgrep ì „ë¶€ í†µê³¼í•œ ì½”ë“œë¥¼ ëŒ€ìƒìœ¼ë¡œ ì‚¼ê° ê²€ì¦ì„ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.
ê²°ê³¼: í‘œì¤€ ë„êµ¬ê°€ ë°œê²¬í•œ ì´ìŠˆ 0ê±´, ì‚¼ê° ê²€ì¦ì´ ë°œê²¬í•œ ì´ìŠˆ 9ê±´.
ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì€ FIFO 100ê±´ ì œí•œ ë¯¸êµ¬í˜„ì…ë‹ˆë‹¤.
ìš”êµ¬ì‚¬í•­ì— 'ìµœëŒ€ 100ê±´, FIFO'ë¼ê³  ìˆ˜ì¹˜ê¹Œì§€ ëª…ì‹œí–ˆì§€ë§Œ, ì½”ë“œëŠ” ë¬´ì œí•œ appendì˜€ìŠµë‹ˆë‹¤.
ruffëŠ” êµ¬ë¬¸ë§Œ, pytestëŠ” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ì—†ìœ¼ë©´ ëª¨ë¥´ê³ , semgrepì€ ë³´ì•ˆ íŒ¨í„´ë§Œ ì²´í¬í•©ë‹ˆë‹¤.
ì‚¼ê° ê²€ì¦ë§Œì´ 'ìš”êµ¬ì‚¬í•­ì— 100ê±´ì´ë¼ ì í˜€ìˆëŠ”ë° ì½”ë“œì—ëŠ” ì—†ë‹¤'ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
í•œê³„ë¡œëŠ” UI ê²€ì¦ì´ ë¶ˆê°€ëŠ¥í•˜ê³ , false positive 22%.
ê°œì„ í•œë‹¤ë©´ Agent Bì— í”„ë¡ íŠ¸ì—”ë“œ ë¶„ì„ skillì„ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤."
-->

---

# Exp 3: Self-Correcting Loop â€” The Automation Pipeline

<div class="text-sm text-gray-400 pb-2">Combining quality gates + triangular verification into one automated loop</div>

```mermaid {scale: 0.8}
flowchart TD
    P["ğŸ“ PROMPT.md + requirements.md"] --> L

    subgraph L["Loop (max N iterations)"]
        direction TB
        P1["<b>Phase 1: Implement</b><br/>Agent A writes code<br/>(or applies feedback)"]
        P2{"<b>Phase 2: Quality Gates</b><br/>ruff â†’ pytest â†’ semgrep<br/>(fast-fail order)"}
        P3{"<b>Phase 3: Triangular Verify</b><br/>Agent B (blind review)<br/>Agent C (discrepancy)"}
        P4["<b>Phase 4: Complete</b><br/>LOOP_COMPLETE"]

        P1 --> P2
        P2 -->|"âŒ fail"| FB1["Error output â†’ feedback"]
        FB1 --> P1
        P2 -->|"âœ… pass"| P3
        P3 -->|"âŒ discrepancy"| FB2["Report â†’ feedback"]
        FB2 --> P1
        P3 -->|"âœ… TRIANGULAR_PASS"| P4
    end

    style P4 fill:#2d5a1e,stroke:#4aff4a
    style FB1 fill:#5a1e1e,stroke:#ff4a4a
    style FB2 fill:#5a1e1e,stroke:#ff4a4a
```

<div class="text-center pt-2 text-sm text-gray-400">

**Core Principle**: Failure = Data â€” failure output becomes the next iteration's input

</div>

<!--
"ì‹¤í—˜ 1ì—ì„œ 'ì—¬ëŸ¬ agentë¥¼ ëŒë¦´ ìˆ˜ ìˆë‹¤', ì‹¤í—˜ 2ì—ì„œ 'agentë¼ë¦¬ ê²€ì¦í•  ìˆ˜ ìˆë‹¤'ë¥¼ í™•ì¸í–ˆìœ¼ë‹ˆ,
ì´ì œ 'ê²€ì¦ë¶€í„° ìˆ˜ì •ê¹Œì§€ ì™„ì „ ìë™í™”'ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
PROMPT.md í•˜ë‚˜ë¡œ ê¸°ëŠ¥ì„ ì •ì˜í•˜ë©´ â€” êµ¬í˜„ â†’ í’ˆì§ˆ ê²Œì´íŠ¸ â†’ ì‚¼ê° ê²€ì¦ â†’ ìˆ˜ì • ê¹Œì§€ ì‚¬ëŒ ê°œì… ì—†ì´ ëŒì•„ê°‘ë‹ˆë‹¤.
í•µì‹¬ ì›ì¹™ì€ 'ì‹¤íŒ¨ = ë°ì´í„°' â€” ì‹¤íŒ¨ ì¶œë ¥ì´ ë‹¤ìŒ ë°˜ë³µì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.
ì´ê±¸ Claude Skillë¡œ íŒ¨í‚¤ì§•í•´ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
-->

---

# Exp 3: Results â€” Automated vs Manual

<div class="text-sm text-gray-400 pb-2">Task: Smart Quote Generator (5 strategies, models + engine + API + tests)</div>

<div class="grid grid-cols-2 gap-8">
<div>

### Quantitative Comparison

| Metric | Automated Loop | Manual Baseline |
|--------|---------------|-----------------|
| Time | 641s | 549s |
| Iterations | 1 | 1 (+retry) |
| Phase 2 failures | 0 | 0 |
| Phase 3 failures | 0 | 1 |
| **Human interventions** | **0** | **1** |
| Tests passing | 81/81 | 82/82 |
| Triangular result | PASS | PASS (2nd try) |

</div>
<div>

### What Happened in Manual

Agent B reviewed the **wrong module** in the first triangular verification attempt.

The automated script avoids this by using `git diff` to **automatically extract the file list** â€” a structural fix, not a prompt fix.

### Key Insight

> **Automation value â‰  speed.** The automated loop was 92s slower, but completed with zero human intervention. The manual run needed me to catch and fix an agent prompt error.

> **If improved**: Streaming output instead of `claude --print` buffering would close the speed gap. The reliability advantage remains.

</div>
</div>

<!--
"Smart Quote Generatorë¥¼ ìë™ ë£¨í”„ì™€ ìˆ˜ë™ìœ¼ë¡œ ê°ê° êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
ìë™ ë£¨í”„ê°€ 92ì´ˆ ëŠë ¸ì§€ë§Œ ì‚¬ëŒ ê°œì… 0ìœ¼ë¡œ ì™„ë£Œ. ìˆ˜ë™ì€ Agent Bê°€ ì˜ëª»ëœ ëª¨ë“ˆì„ ë¦¬ë·°í•´ì„œ ìˆ˜ì •ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤.
ìë™ ìŠ¤í¬ë¦½íŠ¸ëŠ” git diffë¡œ ë³€ê²½ëœ íŒŒì¼ ëª©ë¡ì„ ìë™ ì¶”ì¶œí•˜ë¯€ë¡œ ì´ ë¬¸ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ íšŒí”¼í•©ë‹ˆë‹¤.
ìë™í™”ì˜ ê°€ì¹˜ëŠ” ì†ë„ê°€ ì•„ë‹ˆë¼ ì‹ ë¢°ì„±ì…ë‹ˆë‹¤.
ê°œì„ í•œë‹¤ë©´ claude --printì˜ ë²„í¼ë§ ì˜¤ë²„í—¤ë“œë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°”ê¾¸ë©´ ì†ë„ ì°¨ì´ë„ ì¤„ì–´ë“­ë‹ˆë‹¤."
-->

---

# Exp 4: Pipeline Reusability

<div class="text-sm text-gray-400 pb-2">Same pipeline, completely different domain feature â€” Portfolio Risk Aggregator</div>

<div class="grid grid-cols-2 gap-8">
<div>

### Experiment Design

**Same pipeline:**
```
PROMPT.md â†’ Implement â†’ Quality Gates â†’ Triangular Verify
```

**Different feature:**
- Cross-policy bundle analysis
- Carrier mismatch detection
- Premium concentration check
- Duplicate coverage detection

**Different scope:**
- 5 new files, 182 lines engine code
- 8 test cases
- 3 Pydantic models

</div>
<div>

### Results

| Metric | Exp 3 (Quotes) | Exp 4 (Portfolio) |
|--------|----------------|-------------------|
| Iterations | 1 | 1 |
| Human intervention | 0 | 0 |
| Quality gates | All pass | All pass |
| Triangular | PASS | PASS |
| New tests | 8 | 8 |

<div class="pt-4 text-green-400 font-bold">

Pipeline reusability: PROVEN

</div>

<div class="pt-2 text-sm">

> Just swap `PROMPT.md` â€” the pipeline handles any feature. Packaged as a **Claude Skill** (`self-correcting-loop`) for team reuse.

> **Next step**: Test with cross-module features (e.g., features spanning 3+ layers) to stress-test the pipeline.

</div>
</div>
</div>

<!--
"ì‹¤í—˜ 3ì˜ íŒŒì´í”„ë¼ì¸ì´ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œë„ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.
Portfolio Risk Aggregator â€” ì™„ì „íˆ ë‹¤ë¥¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ê°™ì€ íŒŒì´í”„ë¼ì¸ì— ë„£ì—ˆë”ë‹ˆ
1íšŒ ë°˜ë³µ, ì‚¬ëŒ ê°œì… 0, ëª¨ë“  ê²Œì´íŠ¸ í†µê³¼.
PROMPT.mdë§Œ ë°”ê¾¸ë©´ ì–´ë–¤ ê¸°ëŠ¥ì´ë“  ìë™ êµ¬í˜„ë©ë‹ˆë‹¤.
ì´ê±¸ Claude Skillë¡œ íŒ¨í‚¤ì§•í•´ì„œ íŒ€ì´ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
ê°œì„ í•œë‹¤ë©´, 3ê°œ ë ˆì´ì–´ë¥¼ ë™ì‹œì— ê±´ë“œëŠ” í¬ë¡œìŠ¤ ëª¨ë“ˆ ê¸°ëŠ¥ìœ¼ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸í•˜ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤."
-->

---

# Exp 5: Langfuse LLM Benchmark

<div class="text-sm text-gray-400 pb-2">Data-driven model selection: 3 models Ã— 3 tasks Ã— 5 cases = 45 evaluations</div>

<div class="grid grid-cols-2 gap-6">
<div>

### Accuracy (key_match avg)

| Task | gpt-4o-mini | Sonnet | Haiku |
|------|-------------|--------|-------|
| Risk Signal | 0.70 | **0.90** | 0.80 |
| Endorsement | 0.70 | **1.00** | **1.00** |
| Coverage | 1.00 | 1.00 | 1.00 |
| **Overall** | **0.80** | **0.97** | **0.93** |

### Speed (avg latency)

| | gpt-4o-mini | Sonnet | Haiku |
|--|-------------|--------|-------|
| Avg | **1.7s** | 3.3s | **1.7s** |

### Cost (per 1M tokens in/out)

| | gpt-4o-mini | Sonnet | Haiku |
|--|-------------|--------|-------|
| Price | $0.15/$0.60 | $3/$15 | $0.25/$1.25 |

</div>
<div>

### Key Findings

1. **Task complexity reveals model gaps** â€” simple boolean (coverage) = all equal; complex reasoning (risk signal) = Sonnet wins
2. **gpt-4o-mini under-detects** â€” conservative tendency misses risk signals. In insurance, under-detection > over-detection in danger
3. **Haiku = 90% of Sonnet at 1/10 price** â€” best cost-performance ratio
4. **Prompt v2 pitfall**: fixing one model broke another. Prompt changes need regression testing across all target models

### Recommendation

| Scenario | Model |
|----------|-------|
| Cost-sensitive | gpt-4o-mini |
| Accuracy-first | Sonnet |
| **Best value (recommended)** | **Haiku** |
| Production hybrid | Haiku + Sonnet for risk-signal |

</div>
</div>

<!--
"Langfuse Datasets + Experiments SDKë¡œ 3ê°œ ëª¨ë¸ì„ ì •ëŸ‰ ë¹„êµí–ˆìŠµë‹ˆë‹¤.
ê²°ë¡ : Haikuê°€ Sonnetì˜ 90% ì •í™•ë„ë¥¼ 1/10 ê°€ê²©ì— ì œê³µí•©ë‹ˆë‹¤.
ì¤‘ìš”í•œ ë°œê²¬ì€ gpt-4o-miniê°€ ë¦¬ìŠ¤í¬ ì‹œê·¸ë„ì„ ì ê²Œ ì°¾ëŠ” ê²½í–¥ì´ ìˆë‹¤ëŠ” ê²ƒ.
ë³´í—˜ì—ì„œëŠ” under-detectionì´ over-detectionë³´ë‹¤ ìœ„í—˜í•©ë‹ˆë‹¤.
í”„ë¡¬í”„íŠ¸ v2ë„ ì‹œë„í–ˆëŠ”ë°, í•œ ëª¨ë¸ì„ ê³ ì¹˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ì´ ê¹¨ì§€ëŠ” í˜„ìƒì„ í™•ì¸.
í”„ë¡¬í”„íŠ¸ ë³€ê²½ì€ ë°˜ë“œì‹œ ëª¨ë“  ëŒ€ìƒ ëª¨ë¸ì—ì„œ íšŒê·€ í…ŒìŠ¤íŠ¸ë¥¼ ê±°ì³ì•¼ í•©ë‹ˆë‹¤.
í˜„ì¬ 5ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” ë°©í–¥ì„± í™•ì¸ìš©ì´ê³ , í”„ë¡œë•ì…˜ ê²°ì • ì „ ìµœì†Œ 20ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤."
-->

---

# Architecture: Iterate, Don't Overthink

<div class="text-sm text-gray-400 pb-2">Started fast with flat structure â†’ refactored to hexagonal when boundaries became clear</div>

<div class="grid grid-cols-2 gap-8">
<div>

### Before (Day 1)
```
app/
â”œâ”€â”€ models/        # Pydantic models
â”œâ”€â”€ engine/        # Business logic
â”œâ”€â”€ llm/           # LLM integration
â”œâ”€â”€ routes/        # FastAPI endpoints
â””â”€â”€ templates/     # Jinja2 UI
```
<div class="text-sm text-gray-400 pt-2">
Flat structure â€” fast to build, but domain logic mixed with external dependencies
</div>

</div>
<div>

### After (Day 2)
```
app/
â”œâ”€â”€ domain/        # Pure business logic
â”‚   â”œâ”€â”€ models/    #   (imports nothing external)
â”‚   â”œâ”€â”€ services/  #
â”‚   â””â”€â”€ ports/     #   Protocol interfaces
â”œâ”€â”€ application/   # Use case orchestration
â”œâ”€â”€ api/           # Inbound adapters (FastAPI)
â”œâ”€â”€ adaptor/       # Outbound (LLM, storage, DB)
â””â”€â”€ infra/         # DI wiring
```
<div class="text-sm text-gray-400 pt-2">
Hexagonal â€” BMS changes stay in adaptor/, domain logic untouched
</div>

</div>
</div>

<div class="pt-4 text-sm">

**AI refactored 44 files** (740 added, 578 deleted) **maintaining 100/100 tests** throughout. Applied 4 design patterns: StrEnum, Config centralization, Immutability (frozen models), DI (Depends()). Convention.md updated so Agent B auto-checks pattern compliance in triangular verification.

> *"Architecture is iteration â€” build fast, learn the boundaries, then restructure. AI makes the restructuring cost near-zero."*

</div>

<!--
"ì•„í‚¤í…ì²˜ëŠ” ì •í•˜ê³  â†’ ë°˜ì˜í•˜ê³  â†’ êµì²´í•˜ëŠ” iterationì˜ ë°˜ë³µì´ë¼ ìƒê°í•©ë‹ˆë‹¤.
ì²˜ìŒì— ê¸°ëŠ¥ êµ¬í˜„ë¶€í„° í•˜ê³ ì í•´ì„œ flat êµ¬ì¡°ë¡œ ë¹ ë¥´ê²Œ ë§Œë“¤ì—ˆê³ ,
ê²½ê³„ê°€ ëª…í™•í•´ì§„ ì‹œì ì— í—¥ì‚¬ê³ ë‚ ë¡œ ë¦¬íŒ©í† ë§í–ˆìŠµë‹ˆë‹¤.
AIê°€ 44ê°œ íŒŒì¼ì„ ë¦¬íŒ©í† ë§í•˜ë©´ì„œ 100ê°œ í…ŒìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë„ ê¹¨ëœ¨ë¦¬ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
BMSê°€ ë°”ë€Œì–´ë„ adaptor/ ë ˆì´ì–´ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
ì†”ì§íˆ ì²˜ìŒë¶€í„° ì•„í‚¤í…ì²˜ë¥¼ ì„¸íŒ…í–ˆìœ¼ë©´ ë” ì¢‹ì•˜ì„ í…ë°, ë‹¤ì‹œ í•œë‹¤ë©´ convention.mdì— ë ˆì´ì–´ ê·œì¹™ì„ ì²˜ìŒë¶€í„° ë„£ì–´ë†“ê² ìŠµë‹ˆë‹¤."
-->

---

# How I Think About Rule-based vs LLM

<div class="text-sm text-gray-400 pb-2">LLM is not the default â€” it earns its place through a clear decision framework</div>

<div class="grid grid-cols-2 gap-8">
<div>

### Decision Framework

| Question | â†’ Rule | â†’ LLM |
|----------|--------|-------|
| Is input structured? | âœ… boolean, numeric | free text, notes |
| Is the answer deterministic? | âœ… threshold check | context-dependent |
| Can a simple rule solve it? | âœ… 1-line if | no simple logic |
| Does LLM output feel different? | same quality | âœ… clearly richer |

**If 3+ answers point to Rule â†’ don't use LLM.**

### Where LLM Earned Its Place

| LLM Point | Why It Can't Be Rule-based |
|-----------|---------------------------|
| **Risk Signal Extractor** | Free-text notes â†’ multi-signal reasoning |
| **Endorsement Comparison** | Natural language â†’ material change type |
| **Review Summary** | Multiple flags â†’ coherent 2-3 sentence story |
| **Quote Personalization** | Policy context â†’ tailored broker advice |

</div>
<div>

### The Principle

```
Core pipeline:    100% rule-based (always runs)
LLM layer:        opt-in (RR_LLM_ENABLED flag)
Failure strategy:  graceful fallback to rule-based
User-facing LLM:   4/12 outputs (33%)
Code ratio:         Rule 42% Â· LLM 19% Â· Hybrid 25%
```

<div class="pt-4">

**The core engine works without any LLM.** LLM is an additive layer â€” if every LLM call fails, the system still functions on rule-based results. Users see no difference.

This matters because:
- LLM costs money per call (only 5-15% of policies trigger it)
- LLM latency adds up at batch scale
- Deterministic outputs are easier to test and trust

</div>
</div>
</div>

<div class="pt-2 text-sm text-center text-gray-400">

*"LLM for unstructured text interpretation and multi-context synthesis only. Structured input with deterministic answers â†’ rule-based is more accurate, testable, and cost-effective."*

</div>

<!--
"ì´ í”„ë¡œì íŠ¸ì—ì„œ LLMì„ ì ìš©í•  ë•Œì˜ íŒë‹¨ ê¸°ì¤€ì…ë‹ˆë‹¤.
'LLMì„ ì“¸ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ ì“´ë‹¤'ê°€ ì•„ë‹ˆë¼, 4ê°€ì§€ ì§ˆë¬¸ì„ ë˜ì ¸ì„œ 3ê°œ ì´ìƒ Ruleì„ ê°€ë¦¬í‚¤ë©´ LLMì„ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.
ê²°ê³¼ì ìœ¼ë¡œ 4ê°œ í¬ì¸íŠ¸ë§Œ LLMì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì „ë¶€ ë¹„ì •í˜• í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë£¨ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
ì½”ì–´ ì—”ì§„ì€ 100% rule-basedì´ê³ , LLMì€ ìœ„ì— ì–¹ëŠ” additive layerì…ë‹ˆë‹¤.
LLMì´ ì „ë¶€ ì‹¤íŒ¨í•´ë„ ì‹œìŠ¤í…œì€ ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤.
ì´ê²Œ ë¹„ìš©, ì†ë„, í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„± ì¸¡ë©´ì—ì„œ ê°€ì¥ í•©ë¦¬ì ì¸ êµ¬ì¡°ë¼ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤."
-->

---

# Backend, Domain & Engineering Practices

<div class="grid grid-cols-3 gap-6">
<div>

### DB Integration

```
JSON â†’ Docker Postgres
  â†‘ seed_db.py (8,000)
  â”” fail â†’ JSON fallback
```

- AI agent solved 4 backend issues in sequence (port conflict, async/sync, ORM, serialization)
- **MCP Toolbox**: Agent queries DB directly via SQL â€” no scripts needed
- Production: 3 changes (upsert, ingestion API, cache) â€” pipeline stays the same

</div>
<div>

### Pydantic + LLM Trust

LLM responses = **untrusted external API**:
- 4 Pydantic schemas enforce response format
- Validation failure â†’ rule-based fallback
- Users see no difference on failure

### Domain Knowledge

**Hybrid** (context window is finite):
- `CLAUDE.md`: core terms + pointer
- Custom Skill: full ACORD mapping, gap analysis
- *Like a cache hierarchy â€” always-needed in L1, rest on demand*

</div>
<div>

### Hooks & Skills

| Name | Action |
|------|--------|
| `require-design-doc` | Blocks commit without doc sync |
| `require-experiment-log` | Blocks commit without log |
| `remind-design-doc` | One-time reminder on edit |
| Skill: `insurance-domain` | ACORD field mapping |
| Skill: `self-correcting-loop` | Automated pipeline |

### Architecture (flat â†’ hexagonal)

AI refactored **44 files** maintaining 100/100 tests. Applied StrEnum, Config, Immutability, DI. Domain layer imports zero external modules.

</div>
</div>

<!--
"ëª‡ ê°€ì§€ ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸ì…ë‹ˆë‹¤.
DB â€” Docker + Postgres + SQLAlchemy + MCP ì—°ë™ì„ AI agentê°€ ë°±ì—”ë“œ ì´ìŠˆ 4ê°œë¥¼ ì—°ì‡„ì ìœ¼ë¡œ í•´ê²°í•´ì¤˜ì„œ í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìì¸ ì €ë„ ìˆœì¡°ë¡­ê²Œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.
Pydantic â€” LLM ì‘ë‹µì„ ì™¸ë¶€ APIì²˜ëŸ¼ ì·¨ê¸‰í•´ì„œ ìŠ¤í‚¤ë§ˆë¡œ ê³„ì•½ì„ ê±¸ê³ , ì‹¤íŒ¨ ì‹œ rule-based fallback.
ë„ë©”ì¸ ì§€ì‹ â€” ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ëŠ” ìœ í•œí•œ ìì›ì´ë‹ˆê¹Œ, CLAUDE.mdì—ëŠ” í•µì‹¬ë§Œ, ìƒì„¸ëŠ” Custom Skillì— ë¶„ë¦¬í–ˆìŠµë‹ˆë‹¤.
í›… â€” ì½”ë“œ ë³€ê²½ ì‹œ design-doc ì—†ì´ ì»¤ë°‹ ë¶ˆê°€, ì‹¤í—˜ ë¸Œëœì¹˜ì—ì„œ ë¡œê·¸ ì—†ì´ ì»¤ë°‹ ë¶ˆê°€.
ì•„í‚¤í…ì²˜ â€” flat êµ¬ì¡°ì—ì„œ ì‹œì‘í•´ì„œ ê²½ê³„ê°€ ëª…í™•í•´ì§„ ì‹œì ì— í—¥ì‚¬ê³ ë‚ ë¡œ ë¦¬íŒ©í† ë§í–ˆìŠµë‹ˆë‹¤. AIê°€ 44íŒŒì¼ì„ í…ŒìŠ¤íŠ¸ ê¹¨ì§€ ì•Šê³  ë¦¬íŒ©í† ë§í–ˆìŠµë‹ˆë‹¤."
-->

---

# Skills for the Team

<div class="text-sm text-gray-400 pb-2">Packaging experiments into reusable workflows anyone can adopt</div>

<div class="grid grid-cols-2 gap-8">
<div>

### What I Packaged

**Claude Skill: `self-correcting-loop`**
```
PROMPT.md â†’ Implement â†’ Quality Gates â†’ Triangular Verify
                 â†‘___________ feedback ___________â†“
```
- Any engineer runs it with one command
- Swap `PROMPT.md` for any feature
- Proven on 2 different domain features (Exp 3 & 4)

**Team Guide: `guide-self-correcting-loop.md`**
- Prerequisites, step-by-step usage
- Troubleshooting section
- Shell script + Skill invocation methods

</div>
<div>

### Why This Matters

Skills are **organizational knowledge**, not personal tools.

At Quandri, I've heard that engineers use their own individual skills â€” but good patterns aren't shared across the engineering org yet.

What if the best workflows from one team could be installed by another with a single file?

```
~/.claude/skills/self-correcting-loop/
â””â”€â”€ SKILL.md   â† install this, get the whole pipeline
```

**One engineer experiments â†’ packages it â†’ whole team benefits.**

This is what I want to bring: not just building fast, but making the team faster.

</div>
</div>

<!--
"ì‹¤í—˜ ê²°ê³¼ë¥¼ Skillê³¼ ê°€ì´ë“œë¡œ íŒ¨í‚¤ì§•í–ˆìŠµë‹ˆë‹¤.
self-correcting-loop Skill â€” PROMPT.mdë§Œ ë°”ê¾¸ë©´ ì–´ë–¤ ê¸°ëŠ¥ì´ë“  ìë™ êµ¬í˜„+ê²€ì¦ íŒŒì´í”„ë¼ì¸ì„ ëŒë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê°€ì´ë“œ ë¬¸ì„œë„ ë§Œë“¤ì–´ì„œ íŒ€ì› ëˆ„êµ¬ë‚˜ ë”°ë¼í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤.
Quandriì—ì„œ Chloeì™€ ì´ì•¼ê¸°í–ˆì„ ë•Œ, ì§€ê¸ˆì€ ê° ì—”ì§€ë‹ˆì–´ê°€ ìê¸°ë§Œì˜ Skillì„ ì“°ê³  ìˆê³ 
ì¢‹ì€ íŒ¨í„´ì´ ì—”ì§€ë‹ˆì–´ë§ ì¡°ì§ ì „ì²´ì— ê³µìœ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë“¤ì—ˆìŠµë‹ˆë‹¤.
í•œ ì‚¬ëŒì´ ì‹¤í—˜í•˜ê³ , íŒ¨í‚¤ì§•í•˜ê³ , íŒ€ ì „ì²´ê°€ ì“¸ ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê²ƒ â€” ì´ê²Œ ì œê°€ ê°€ì ¸ê°€ê³  ì‹¶ì€ ë°©ì‹ì…ë‹ˆë‹¤.
ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ê²ƒë¿ ì•„ë‹ˆë¼, íŒ€ ì „ì²´ë¥¼ ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ê²ƒ."
-->

---
layout: center
---

# What I'd Improve

<div class="grid grid-cols-2 gap-8 pt-4">
<div>

### Limitations Identified

- **Triangular verification blind to UI** â€” Agent B only reads Python; templates unchecked
- **5 test cases insufficient** for statistically significant LLM benchmark â€” need 20+
- **Prompt v2 cross-model regression** â€” improving one model can degrade another
- **Self-correcting loop speed** â€” `claude --print` buffering adds ~90s overhead
- **Hexagonal was added late** â€” should have been in convention.md from day 1

</div>
<div>

### If I Had More Time

- Add **frontend analysis skill** to Agent B for template/JS verification
- Expand Langfuse test cases to **30+** for production model selection
- Build **model-specific prompt variants** instead of one-size-fits-all
- Add **streaming output** to self-correcting loop for real-time feedback
- Create **architecture-aware skill** that enforces layer boundaries from the start
- Implement **batch ingestion API** (`POST /ingest/batch`) for production BMS integration

</div>
</div>

<div class="pt-8 text-center text-xl">

> *"Agent-native isn't about delegating code to AI. It's about building an environment where AI can do its best work â€” rulesets, quality gates, clear plans. Then a week of work becomes two days."*

</div>

<!--
"í•œê³„ì™€ ê°œì„  ë°©í–¥ì…ë‹ˆë‹¤.
ì‚¼ê° ê²€ì¦ì€ UIë¥¼ ëª» ë´…ë‹ˆë‹¤ â€” í”„ë¡ íŠ¸ì—”ë“œ ë¶„ì„ skillì„ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤.
Langfuse í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5ê°œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤ â€” í”„ë¡œë•ì…˜ ê²°ì • ì „ 30ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.
í”„ë¡¬í”„íŠ¸ ê°œì„ ì€ ëª¨ë“  ëŒ€ìƒ ëª¨ë¸ì—ì„œ íšŒê·€ í…ŒìŠ¤íŠ¸ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
í—¥ì‚¬ê³ ë‚  ì•„í‚¤í…ì²˜ëŠ” ëŠ¦ê²Œ ì¶”ê°€í–ˆëŠ”ë°, ë‹¤ì‹œ í•œë‹¤ë©´ ì²˜ìŒë¶€í„° convention.mdì— ë„£ê² ìŠµë‹ˆë‹¤.
Agent-nativeëŠ” AIì—ê²Œ ì½”ë“œë¥¼ ë§¡ê¸°ëŠ” ê²Œ ì•„ë‹ˆë¼, AIê°€ ì˜ ì¼í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤."
-->

---
layout: center
class: text-center
---

# Q & A

<div class="pt-8 text-gray-400">

**Tech Stack**: Python 3.13 Â· FastAPI Â· Pydantic v2 Â· SQLAlchemy Â· Docker Â· PostgreSQL

**AI Tools**: Claude Code Â· Langfuse Â· MCP Toolbox Â· Custom Skills & Hooks

**Metrics**: 100 tests Â· 14+ endpoints Â· 8,000 policies < 1s Â· ~2,500 lines Â· 2 days

</div>

<div class="pt-12 text-sm text-gray-500">

github.com/[repo] Â· Yeonsu Choi

</div>

<!--
"ê°ì‚¬í•©ë‹ˆë‹¤. ì§ˆë¬¸ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."

ì˜ˆìƒ Q&A:

Q1: "ì‚¼ê°ê²€ì¦ìœ¼ë¡œ êµ¬í˜„í•  ê¸°ëŠ¥ë“¤ì„ íì— ì–´ë–»ê²Œ ë„£ê³  ì–´ë–»ê²Œ ë³´ë‚˜ìš”?"
A: "ë””ë ‰í† ë¦¬ ê¸°ë°˜ì…ë‹ˆë‹¤. docs/experiments/ì— requirements + PROMPT íŒŒì¼ì„ ë„£ìœ¼ë©´ íŒŒì´í”„ë¼ì¸ì´ ì½ì–´ê°‘ë‹ˆë‹¤.
ì‹¤í–‰ ë¡œê·¸ëŠ” docs/logs/loop-execution.logì—, ê²€ì¦ ê²°ê³¼ëŠ” blind-review.mdì™€ discrepancy-report.mdì— ìë™ ìƒì„±ë©ë‹ˆë‹¤.
íŒ€ì—ì„œ ì“¸ ë•ŒëŠ” ê°€ì´ë“œ ë¬¸ì„œ(guide-self-correcting-loop.md)ë¥¼ ë”°ë¥´ë©´ ë©ë‹ˆë‹¤."

Q2: "ë°ì´í„° 8,000ê±´ì´ ëì´ ì•„ë‹ˆë¼ ê³„ì† ë“¤ì–´ì˜¬í…ë° ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
A: "ì‹¤ì œë¡œëŠ” ë³´í—˜ì‚¬ê°€ ê°±ì‹  ì‹œì¦Œì— BMSë¡œ ë°°ì¹˜ ë°ì´í„°ë¥¼ ë‚´ë ¤ë³´ë‚´ê³  Epic SDKë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
ë°”ê¿”ì•¼ í•  ê±´ 3ê°€ì§€ â€” upsertë¡œ ì¤‘ë³µ ë°©ì§€, ë°°ì¹˜ ingestion API ì¶”ê°€, ìºì‹œ ë¬´íš¨í™”.
ë¶„ì„ íŒŒì´í”„ë¼ì¸(diff â†’ flag â†’ risk)ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
DataSourcePort ì¸í„°í˜ì´ìŠ¤ ë•ë¶„ì— JSON â†’ DB ì „í™˜ë„ ì½”ë“œ 1íŒŒì¼ ë³€ê²½ì´ì—ˆìŠµë‹ˆë‹¤."

Q3: "í”„ë¡ íŠ¸ì—”ë“œ ë°°ê²½ì¸ë° ë°±ì—”ë“œëŠ” ì–´ë–»ê²Œ?"
A: "ì†”ì§íˆ Docker + PostgreSQL + SQLAlchemyëŠ” ì²˜ìŒì´ì—ˆìŠµë‹ˆë‹¤.
AI agentê°€ í¬íŠ¸ ì¶©ëŒ, async/sync í˜¸í™˜, ORM ë§¤í•‘ ë¬¸ì œë¥¼ ì—°ì‡„ì ìœ¼ë¡œ ì§„ë‹¨í•˜ê³  ìˆ˜ì •í•´ì¤˜ì„œ ìˆœì¡°ë¡­ê²Œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.
ì´ê²Œ agent-nativeì˜ ì¥ì ì…ë‹ˆë‹¤ â€” ëª¨ë¥´ëŠ” ì˜ì—­ë„ agentì™€ í•¨ê»˜ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
-->
